{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T16:54:24.063454Z",
     "start_time": "2024-11-30T16:52:55.428770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix, remove_self_loops\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('train.txt', sep=';', header=None, names=['text', 'label'])\n",
    "texts = data['text'].tolist()\n",
    "\n",
    "# Encode labels into numeric format\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(data['label'])\n",
    "\n",
    "# Generate sentence embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Pre-trained Sentence-BERT\n",
    "embeddings = model.encode(texts)  # Shape: (n, 384)\n",
    "\n",
    "# Calculate edge weights using cosine similarity\n",
    "edge_weights = cosine_similarity(embeddings)\n",
    "threshold = 0.7  # Define a similarity threshold\n",
    "adjacency_matrix = (edge_weights > threshold).astype(int)\n",
    "\n",
    "# Construct graph structure\n",
    "x = torch.tensor(embeddings, dtype=torch.float)\n",
    "adj_matrix = coo_matrix(adjacency_matrix)\n",
    "edge_index, edge_attr = from_scipy_sparse_matrix(adj_matrix)\n",
    "edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
    "\n",
    "# Create graph data in PyTorch Geometric format\n",
    "graph_data = Data(\n",
    "    x=x,\n",
    "    edge_index=edge_index,\n",
    "    edge_attr=torch.tensor(edge_attr.clone().detach(), dtype=torch.float),\n",
    "    y=torch.tensor(labels, dtype=torch.long)  # Encoded emotion labels\n",
    ")\n",
    "\n",
    "# Create and save the converted dataset for inspection\n",
    "# Combine embeddings, texts, and labels into a DataFrame\n",
    "embeddings_df = pd.DataFrame(embeddings)  # Add embeddings as columns\n",
    "embeddings_df['text'] = texts  # Add original text\n",
    "embeddings_df['label'] = data['label']  # Add emotion labels\n",
    "embeddings_df['encoded_label'] = labels  # Add encoded labels\n",
    "\n",
    "# Save the DataFrame to a CSV file for inspection\n",
    "embeddings_df.to_csv('converted_dataset.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(embeddings_df.head())\n",
    "\n",
    "\n"
   ],
   "id": "d11d5db99486b1fb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zekej\\PycharmProjects\\pythonProject1\\.venv\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Users\\zekej\\PycharmProjects\\pythonProject1\\.venv\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Users\\zekej\\AppData\\Local\\Temp\\ipykernel_11172\\562738523.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_attr=torch.tensor(edge_attr.clone().detach(), dtype=torch.float),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6  \\\n",
      "0 -0.055051 -0.007697  0.063530 -0.039664  0.116901 -0.123296  0.058080   \n",
      "1  0.009239 -0.052964  0.019263  0.034021  0.125202  0.027428  0.077058   \n",
      "2 -0.074503 -0.010642 -0.003460 -0.073246 -0.018509 -0.026024  0.023560   \n",
      "3  0.108594  0.095322  0.036477  0.015178  0.089073 -0.012647 -0.089686   \n",
      "4 -0.016712 -0.078771  0.032170 -0.053829  0.115593 -0.051190  0.132093   \n",
      "\n",
      "          7         8         9  ...       377       378       379       380  \\\n",
      "0  0.067705  0.071730 -0.109816  ...  0.021249 -0.029084  0.084679  0.016152   \n",
      "1  0.035879  0.075603 -0.052699  ...  0.132352 -0.082222  0.003469  0.095559   \n",
      "2  0.062387  0.110395  0.064938  ...  0.019752  0.078386 -0.010269  0.041514   \n",
      "3 -0.070015  0.042590 -0.011443  ...  0.023587  0.056529  0.024166  0.103731   \n",
      "4  0.037378  0.001562 -0.072058  ... -0.016146  0.007182  0.029738  0.059137   \n",
      "\n",
      "        381       382       383  \\\n",
      "0  0.015425 -0.135161 -0.064534   \n",
      "1 -0.060182 -0.027176 -0.026275   \n",
      "2 -0.024779 -0.042020  0.024512   \n",
      "3 -0.044091 -0.109329  0.034851   \n",
      "4 -0.062703 -0.019559 -0.057704   \n",
      "\n",
      "                                                text    label  encoded_label  \n",
      "0                            i didnt feel humiliated  sadness              4  \n",
      "1  i can go from feeling so hopeless to so damned...  sadness              4  \n",
      "2   im grabbing a minute to post i feel greedy wrong    anger              0  \n",
      "3  i am ever feeling nostalgic about the fireplac...     love              3  \n",
      "4                               i am feeling grouchy    anger              0  \n",
      "\n",
      "[5 rows x 387 columns]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T16:54:24.116866Z",
     "start_time": "2024-11-30T16:54:24.063454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class EmotionGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.5):\n",
    "        super(EmotionGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)  # First GCN layer\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)  # Second GCN layer\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)  # Fully connected output layer\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Graph convolution + ReLU + Dropout\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Second graph convolution + ReLU\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)  # Log probabilities for classification\n"
   ],
   "id": "fb29531d69230aa3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T16:54:24.257865Z",
     "start_time": "2024-11-30T16:54:24.126392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define hyperparameters\n",
    "input_dim = 384  # Dimension of sentence embeddings\n",
    "hidden_dim = 128  # Hidden layer size\n",
    "output_dim = len(torch.unique(graph_data.y))  # Number of classes (emotions)\n",
    "learning_rate = 0.01\n",
    "dropout = 0.5\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = EmotionGNN(input_dim, hidden_dim, output_dim, dropout).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "criterion = torch.nn.NLLLoss()  # Negative Log-Likelihood Loss for multi-class classification\n"
   ],
   "id": "708d005048e3b6b9",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T16:54:39.328636Z",
     "start_time": "2024-11-30T16:54:24.257865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Move data to the device\n",
    "graph_data = graph_data.to(device)\n",
    "\n",
    "# Training loop\n",
    "def train():\n",
    "    model.train()  # Set to training mode\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    out = model(graph_data.x, graph_data.edge_index)  # Forward pass\n",
    "    loss = criterion(out, graph_data.y)  # Compute loss\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update model parameters\n",
    "    return loss.item()\n",
    "\n",
    "# Validation function\n",
    "def validate():\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        out = model(graph_data.x, graph_data.edge_index)  # Forward pass\n",
    "        val_loss = criterion(out, graph_data.y)  # Compute validation loss\n",
    "        pred = out.argmax(dim=1)  # Get predictions\n",
    "        accuracy = (pred == graph_data.y).sum().item() / graph_data.y.size(0)  # Compute accuracy\n",
    "    return val_loss.item(), accuracy\n",
    "\n",
    "# Run training for several epochs\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    loss = train()\n",
    "    val_loss, val_acc = validate()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
   ],
   "id": "78b2b2bd2cf70e38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 1.7815, Val Loss: 1.6831, Val Acc: 0.3457\n",
      "Epoch 2/100, Loss: 1.6798, Val Loss: 1.5667, Val Acc: 0.4376\n",
      "Epoch 3/100, Loss: 1.5702, Val Loss: 1.5709, Val Acc: 0.5036\n",
      "Epoch 4/100, Loss: 1.5805, Val Loss: 1.4705, Val Acc: 0.5086\n",
      "Epoch 5/100, Loss: 1.4804, Val Loss: 1.4301, Val Acc: 0.5118\n",
      "Epoch 6/100, Loss: 1.4360, Val Loss: 1.3850, Val Acc: 0.5153\n",
      "Epoch 7/100, Loss: 1.3922, Val Loss: 1.3201, Val Acc: 0.5173\n",
      "Epoch 8/100, Loss: 1.3292, Val Loss: 1.2651, Val Acc: 0.5196\n",
      "Epoch 9/100, Loss: 1.2768, Val Loss: 1.2415, Val Acc: 0.5228\n",
      "Epoch 10/100, Loss: 1.2607, Val Loss: 1.2133, Val Acc: 0.5301\n",
      "Epoch 11/100, Loss: 1.2304, Val Loss: 1.1751, Val Acc: 0.5507\n",
      "Epoch 12/100, Loss: 1.1950, Val Loss: 1.1461, Val Acc: 0.5887\n",
      "Epoch 13/100, Loss: 1.1611, Val Loss: 1.1255, Val Acc: 0.6259\n",
      "Epoch 14/100, Loss: 1.1413, Val Loss: 1.0987, Val Acc: 0.6330\n",
      "Epoch 15/100, Loss: 1.1168, Val Loss: 1.0617, Val Acc: 0.6344\n",
      "Epoch 16/100, Loss: 1.0857, Val Loss: 1.0334, Val Acc: 0.6350\n",
      "Epoch 17/100, Loss: 1.0562, Val Loss: 1.0141, Val Acc: 0.6389\n",
      "Epoch 18/100, Loss: 1.0449, Val Loss: 0.9942, Val Acc: 0.6456\n",
      "Epoch 19/100, Loss: 1.0243, Val Loss: 0.9787, Val Acc: 0.6445\n",
      "Epoch 20/100, Loss: 1.0131, Val Loss: 0.9640, Val Acc: 0.6492\n",
      "Epoch 21/100, Loss: 0.9994, Val Loss: 0.9524, Val Acc: 0.6556\n",
      "Epoch 22/100, Loss: 0.9866, Val Loss: 0.9429, Val Acc: 0.6630\n",
      "Epoch 23/100, Loss: 0.9735, Val Loss: 0.9272, Val Acc: 0.6674\n",
      "Epoch 24/100, Loss: 0.9517, Val Loss: 0.9162, Val Acc: 0.6696\n",
      "Epoch 25/100, Loss: 0.9454, Val Loss: 0.9065, Val Acc: 0.6733\n",
      "Epoch 26/100, Loss: 0.9339, Val Loss: 0.8954, Val Acc: 0.6767\n",
      "Epoch 27/100, Loss: 0.9213, Val Loss: 0.8857, Val Acc: 0.6806\n",
      "Epoch 28/100, Loss: 0.9128, Val Loss: 0.8763, Val Acc: 0.6871\n",
      "Epoch 29/100, Loss: 0.9068, Val Loss: 0.8658, Val Acc: 0.6927\n",
      "Epoch 30/100, Loss: 0.8968, Val Loss: 0.8550, Val Acc: 0.6959\n",
      "Epoch 31/100, Loss: 0.8865, Val Loss: 0.8452, Val Acc: 0.6961\n",
      "Epoch 32/100, Loss: 0.8745, Val Loss: 0.8362, Val Acc: 0.6991\n",
      "Epoch 33/100, Loss: 0.8624, Val Loss: 0.8260, Val Acc: 0.7044\n",
      "Epoch 34/100, Loss: 0.8515, Val Loss: 0.8173, Val Acc: 0.7077\n",
      "Epoch 35/100, Loss: 0.8532, Val Loss: 0.8084, Val Acc: 0.7091\n",
      "Epoch 36/100, Loss: 0.8440, Val Loss: 0.8000, Val Acc: 0.7094\n",
      "Epoch 37/100, Loss: 0.8352, Val Loss: 0.7923, Val Acc: 0.7117\n",
      "Epoch 38/100, Loss: 0.8227, Val Loss: 0.7833, Val Acc: 0.7183\n",
      "Epoch 39/100, Loss: 0.8122, Val Loss: 0.7750, Val Acc: 0.7211\n",
      "Epoch 40/100, Loss: 0.8119, Val Loss: 0.7669, Val Acc: 0.7224\n",
      "Epoch 41/100, Loss: 0.8036, Val Loss: 0.7585, Val Acc: 0.7245\n",
      "Epoch 42/100, Loss: 0.7946, Val Loss: 0.7498, Val Acc: 0.7298\n",
      "Epoch 43/100, Loss: 0.7852, Val Loss: 0.7417, Val Acc: 0.7326\n",
      "Epoch 44/100, Loss: 0.7864, Val Loss: 0.7329, Val Acc: 0.7352\n",
      "Epoch 45/100, Loss: 0.7711, Val Loss: 0.7250, Val Acc: 0.7375\n",
      "Epoch 46/100, Loss: 0.7664, Val Loss: 0.7163, Val Acc: 0.7409\n",
      "Epoch 47/100, Loss: 0.7598, Val Loss: 0.7090, Val Acc: 0.7442\n",
      "Epoch 48/100, Loss: 0.7456, Val Loss: 0.7008, Val Acc: 0.7443\n",
      "Epoch 49/100, Loss: 0.7426, Val Loss: 0.6925, Val Acc: 0.7495\n",
      "Epoch 50/100, Loss: 0.7352, Val Loss: 0.6836, Val Acc: 0.7573\n",
      "Epoch 51/100, Loss: 0.7279, Val Loss: 0.6751, Val Acc: 0.7602\n",
      "Epoch 52/100, Loss: 0.7216, Val Loss: 0.6675, Val Acc: 0.7654\n",
      "Epoch 53/100, Loss: 0.7179, Val Loss: 0.6595, Val Acc: 0.7694\n",
      "Epoch 54/100, Loss: 0.7099, Val Loss: 0.6513, Val Acc: 0.7696\n",
      "Epoch 55/100, Loss: 0.7011, Val Loss: 0.6441, Val Acc: 0.7708\n",
      "Epoch 56/100, Loss: 0.6959, Val Loss: 0.6369, Val Acc: 0.7754\n",
      "Epoch 57/100, Loss: 0.6902, Val Loss: 0.6332, Val Acc: 0.7759\n",
      "Epoch 58/100, Loss: 0.6829, Val Loss: 0.6371, Val Acc: 0.7722\n",
      "Epoch 59/100, Loss: 0.6912, Val Loss: 0.6348, Val Acc: 0.7722\n",
      "Epoch 60/100, Loss: 0.6875, Val Loss: 0.6167, Val Acc: 0.7849\n",
      "Epoch 61/100, Loss: 0.6714, Val Loss: 0.6075, Val Acc: 0.7852\n",
      "Epoch 62/100, Loss: 0.6666, Val Loss: 0.6111, Val Acc: 0.7827\n",
      "Epoch 63/100, Loss: 0.6676, Val Loss: 0.5983, Val Acc: 0.7936\n",
      "Epoch 64/100, Loss: 0.6599, Val Loss: 0.5896, Val Acc: 0.7939\n",
      "Epoch 65/100, Loss: 0.6506, Val Loss: 0.5889, Val Acc: 0.7958\n",
      "Epoch 66/100, Loss: 0.6544, Val Loss: 0.5798, Val Acc: 0.8027\n",
      "Epoch 67/100, Loss: 0.6420, Val Loss: 0.5734, Val Acc: 0.7987\n",
      "Epoch 68/100, Loss: 0.6318, Val Loss: 0.5716, Val Acc: 0.8007\n",
      "Epoch 69/100, Loss: 0.6334, Val Loss: 0.5608, Val Acc: 0.8107\n",
      "Epoch 70/100, Loss: 0.6253, Val Loss: 0.5582, Val Acc: 0.8064\n",
      "Epoch 71/100, Loss: 0.6212, Val Loss: 0.5515, Val Acc: 0.8082\n",
      "Epoch 72/100, Loss: 0.6210, Val Loss: 0.5421, Val Acc: 0.8174\n",
      "Epoch 73/100, Loss: 0.6103, Val Loss: 0.5416, Val Acc: 0.8136\n",
      "Epoch 74/100, Loss: 0.6108, Val Loss: 0.5338, Val Acc: 0.8171\n",
      "Epoch 75/100, Loss: 0.5983, Val Loss: 0.5267, Val Acc: 0.8224\n",
      "Epoch 76/100, Loss: 0.5998, Val Loss: 0.5241, Val Acc: 0.8221\n",
      "Epoch 77/100, Loss: 0.5971, Val Loss: 0.5147, Val Acc: 0.8243\n",
      "Epoch 78/100, Loss: 0.5886, Val Loss: 0.5114, Val Acc: 0.8264\n",
      "Epoch 79/100, Loss: 0.5887, Val Loss: 0.5073, Val Acc: 0.8297\n",
      "Epoch 80/100, Loss: 0.5830, Val Loss: 0.5006, Val Acc: 0.8318\n",
      "Epoch 81/100, Loss: 0.5759, Val Loss: 0.4978, Val Acc: 0.8346\n",
      "Epoch 82/100, Loss: 0.5712, Val Loss: 0.4912, Val Acc: 0.8346\n",
      "Epoch 83/100, Loss: 0.5709, Val Loss: 0.4848, Val Acc: 0.8365\n",
      "Epoch 84/100, Loss: 0.5629, Val Loss: 0.4821, Val Acc: 0.8413\n",
      "Epoch 85/100, Loss: 0.5649, Val Loss: 0.4769, Val Acc: 0.8394\n",
      "Epoch 86/100, Loss: 0.5538, Val Loss: 0.4707, Val Acc: 0.8452\n",
      "Epoch 87/100, Loss: 0.5500, Val Loss: 0.4682, Val Acc: 0.8461\n",
      "Epoch 88/100, Loss: 0.5516, Val Loss: 0.4635, Val Acc: 0.8438\n",
      "Epoch 89/100, Loss: 0.5428, Val Loss: 0.4578, Val Acc: 0.8486\n",
      "Epoch 90/100, Loss: 0.5433, Val Loss: 0.4534, Val Acc: 0.8494\n",
      "Epoch 91/100, Loss: 0.5406, Val Loss: 0.4469, Val Acc: 0.8526\n",
      "Epoch 92/100, Loss: 0.5389, Val Loss: 0.4446, Val Acc: 0.8548\n",
      "Epoch 93/100, Loss: 0.5333, Val Loss: 0.4429, Val Acc: 0.8534\n",
      "Epoch 94/100, Loss: 0.5232, Val Loss: 0.4360, Val Acc: 0.8570\n",
      "Epoch 95/100, Loss: 0.5337, Val Loss: 0.4326, Val Acc: 0.8588\n",
      "Epoch 96/100, Loss: 0.5206, Val Loss: 0.4282, Val Acc: 0.8563\n",
      "Epoch 97/100, Loss: 0.5208, Val Loss: 0.4245, Val Acc: 0.8629\n",
      "Epoch 98/100, Loss: 0.5148, Val Loss: 0.4243, Val Acc: 0.8634\n",
      "Epoch 99/100, Loss: 0.5168, Val Loss: 0.4204, Val Acc: 0.8652\n",
      "Epoch 100/100, Loss: 0.5141, Val Loss: 0.4143, Val Acc: 0.8679\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T16:54:50.974011Z",
     "start_time": "2024-11-30T16:54:39.328636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "#  Load the test dataset\n",
    "test_data = pd.read_csv('test.txt', sep=';', header=None, names=['text', 'label'])\n",
    "test_texts = test_data['text'].tolist()\n",
    "test_labels = test_data['label'].tolist()\n",
    "\n",
    "#  Generate embeddings for the test set using SentenceTransformer\n",
    "sentence_transformer_model = SentenceTransformer('all-MiniLM-L6-v2')  # Pre-trained model\n",
    "test_embeddings = sentence_transformer_model.encode(test_texts)  # Generate embeddings\n",
    "\n",
    "#  Encode labels using the LabelEncoder from training\n",
    "test_encoded_labels = label_encoder.transform(test_labels)  \n",
    "\n",
    "#  Convert embeddings and labels to PyTorch tensors\n",
    "test_x = torch.tensor(test_embeddings, dtype=torch.float).to(device)  # Embeddings\n",
    "test_y = torch.tensor(test_encoded_labels, dtype=torch.long).to(device)  # Encoded labels\n",
    "\n",
    "# Rebuild the graph for the test set\n",
    "# Compute cosine similarity between test embeddings\n",
    "test_edge_weights = cosine_similarity(test_embeddings)\n",
    "\n",
    "# Apply a threshold to create adjacency matrix\n",
    "test_threshold = 0.7  # Adjust the threshold\n",
    "test_adjacency_matrix = (test_edge_weights > test_threshold).astype(int)\n",
    "\n",
    "# Create edge_index for the test set\n",
    "test_adj_matrix = coo_matrix(test_adjacency_matrix)\n",
    "test_edge_index, test_edge_attr = from_scipy_sparse_matrix(test_adj_matrix)\n",
    "\n",
    "# Convert edge_index and edge_attr to PyTorch tensors\n",
    "test_edge_index = test_edge_index.to(device)\n",
    "test_edge_attr = torch.tensor(test_edge_attr, dtype=torch.float).to(device)\n",
    "\n",
    "# Evaluate the GNN model\n",
    "model.eval()  \n",
    "with torch.no_grad():\n",
    "    # Forward pass through the GNN\n",
    "    out = model(test_x, test_edge_index)\n",
    "    predictions = out.argmax(dim=1)  # Get predicted labels\n",
    "\n",
    "# Convert predictions and true labels to CPU for sklearn compatibility\n",
    "true_labels = test_y.cpu().numpy()\n",
    "predicted_labels = predictions.cpu().numpy()\n",
    "\n",
    "# Compute overall accuracy\n",
    "overall_accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# Compute precision, recall, F1-score, and support (per-label)\n",
    "metrics = precision_recall_fscore_support(true_labels, predicted_labels, labels=range(len(label_encoder.classes_)))\n",
    "\n",
    "# Create a detailed metrics table\n",
    "detailed_metrics = pd.DataFrame({\n",
    "    \"Label\": label_encoder.classes_,\n",
    "    \"Accuracy\": [(true_labels[predicted_labels == i] == i).sum() / (true_labels == i).sum() for i in range(len(label_encoder.classes_))],\n",
    "    \"Precision\": metrics[0],\n",
    "    \"Recall\": metrics[1],\n",
    "    \"F1-Score\": metrics[2],\n",
    "    \"Support\": metrics[3]\n",
    "})\n",
    "\n",
    "# Add overall accuracy \n",
    "detailed_metrics = pd.concat([\n",
    "    detailed_metrics,\n",
    "    pd.DataFrame({\n",
    "        \"Label\": [\"Overall\"],\n",
    "        \"Accuracy\": [overall_accuracy],\n",
    "        \"Precision\": [None],  # Not meaningful for overall\n",
    "        \"Recall\": [None],     # Not meaningful for overall\n",
    "        \"F1-Score\": [None],   # Not meaningful for overall\n",
    "        \"Support\": [len(true_labels)]\n",
    "    })\n",
    "], ignore_index=True)\n",
    "\n",
    "# Print the detailed table\n",
    "print(detailed_metrics)\n",
    "\n",
    "# Save metrics to a CSV file for reference\n",
    "detailed_metrics.to_csv('test_metrics.csv', index=False)\n"
   ],
   "id": "b77b5f2493f84003",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Label  Accuracy  Precision    Recall  F1-Score  Support\n",
      "0     anger  0.683636   0.712121  0.683636  0.697588      275\n",
      "1      fear  0.651786   0.733668  0.651786  0.690307      224\n",
      "2       joy  0.834532   0.745501  0.834532  0.787508      695\n",
      "3      love  0.484277   0.550000  0.484277  0.515050      159\n",
      "4   sadness  0.769363   0.776042  0.769363  0.772688      581\n",
      "5  surprise  0.393939   0.604651  0.393939  0.477064       66\n",
      "6   Overall  0.732000        NaN       NaN       NaN     2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zekej\\AppData\\Local\\Temp\\ipykernel_11172\\245782076.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_edge_attr = torch.tensor(test_edge_attr, dtype=torch.float).to(device)\n",
      "C:\\Users\\zekej\\AppData\\Local\\Temp\\ipykernel_11172\\245782076.py:70: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  detailed_metrics = pd.concat([\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
